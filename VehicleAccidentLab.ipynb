{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Hands On Lab - Python\n",
    "\n",
    "#### Let's first down load the data that we're going to analyze from GitHub. For purposes of this lab, you'll just land the data on temporary storage. In practice, the data would reside on object storage or in a database.\n",
    "\n",
    "#### The data file contains vehicle accident data and has a size of about 6.5 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we'll read the file into a SparkData frame. We specify that the file is in csv format and that the first row of the file contains column names. We are also asking Spark to infer the schema and assign data types. We'll then print out the first two rows of the DataFrame.\n",
    "\n",
    "#### Here's how you do it.\n",
    "\n",
    "    df_data_1 = spark.read.format('csv').options(header='true', inferschema='true').load(\"ACCIDENT2007-FullDataSet.csv\")\n",
    "    df_data_1.take(2)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the infered schema of the resulting DataFrame.\n",
    "\n",
    "    df_data_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you have a DataFrame object, you can do analysis on it such as calculating the correlation between various variables.\n",
    "\n",
    "#### You can see the correlation between whether the individual was drunk and if the accident resulted in fatalities by performing a simple Pearson correlation on two variables. \n",
    "\n",
    "    df_data_1.corr('DRUNK_DR','FATALS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also mport your favorite libraries to use within your notebook, for example, the popular Pandas Python library. \n",
    "\n",
    "#### Import the pandas library, use it to convert the Spark DataFrame to a Pandas DataFrame, and use the head function to show the top 5 rows values.\n",
    "\n",
    "    import pandas as pd\n",
    "    pd_fars = df_data_1.toPandas()\n",
    "    pd_fars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also get summary statistics using the describe function.  This can help you determine missing values and the distribution of your attributes.    \n",
    "\n",
    "    pd_fars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to look at an individual states worth of data.  The Spark DataFrame object supports a filter option to allow you to filter the data based on a column of interest (ex. STATE) and the resulting value. The Fatality Analysis Reporting System (FARS) Analytical User's Manual provides a reference to the state codes (https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812092) .  In this example, we will look at the state of California - state code CA.  \n",
    "\n",
    "#### Next., convert the results to a new Panda dataframe and view the first 5 rows data. \n",
    "\n",
    "#### The code should look like this.\n",
    "\n",
    "    df_cal=df_data_1.filter(df_data_1['STATE']==6)\n",
    "    pd_cal=df_cal.toPandas()\n",
    "    pd_cal.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL enables applications to run SQL queries programmatically and return the result as a DataFrame. Let's perform the same query we did above, but this time using SQL.\n",
    "\n",
    "#### First you need to create a Temporary Table\n",
    "\n",
    "    df_data_1_tempTable = df_data_1.registerTempTable(\"tempTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now run a similar query as above using SQL, but this time only selecting a subset of the columns for which to return results. Note that the query result are the same as above, but with the SQL result only returning the subset of columns that were selected.\n",
    "\n",
    "    sql = \"select STATE, COUNTY, MONTH, DAY, HOUR, MINUTE, VE_TOTAL, PERSONS, PEDS, NHS from tempTable where STATE = 6\"\n",
    "    df_cal2 = spark.sql(sql)\n",
    "    pd_cal2=df_cal2.toPandas()\n",
    "    pd_cal2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Let's now map out the occurrences for the state of interest (CA). Note that the LATITUDE and LONGITUD values were inferred as integers. These need to be converted to float. We can create new columns for a modified version of these fields so that we can map the individual occurances on a map.\n",
    "\n",
    "#### Use lamba (anonymous) functions to create a lon and lat column that represents the expected values.\n",
    "\n",
    "    pd_cal['lat'] = pd_cal['LATITUDE'].map(lambda x: (x * 1.0) / 1000000)\n",
    "    pd_cal['lon'] = pd_cal['LONGITUD'].map(lambda x: (x * -1.0) / 1000000)\n",
    "    pd_cal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the data we need, we can import the Brunel package and use it to show a graphical map of the occurances. The Brunel Visualization Language is a high-level language developed and open-sourced by IBM. Brunel describes visualizations in terms of composable actions, and drives a visualization engine (D3) that performs the actual rendering and interactivity. \n",
    "\n",
    "#### Use the following code to display the map for your state using the lon and lat values and use PERSONS to display a color scale based on the number of individuals involved in the accident.\n",
    "\n",
    "    import brunel\n",
    "    %brunel map ('CA') + data('pd_cal') x(lon) y(lat) color(PERSONS) tooltip(VE_TOTAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PixieDust is an open source Python helper library developed by IBM that works as an add-on to Jupyter notebooks to improve the user experience of working with data. Pixie dust provides an easy way to visualize the data using various table, charts, and maps.  \n",
    "\n",
    "To import the PixieDust package, you simply need to use and import statement. Once imported, you bring up the interactive display area by using the display function on your dataset.\n",
    "    \n",
    "    from pixiedust.display import *\n",
    "    display(df_data_1)\n",
    "\n",
    "The initial display is a table view of the dataframe.  \n",
    "\n",
    "Switch views to the pie chart by selecting the middle charting drop down menu at the top left of the display area. This will display a pie chart of the count of accidents by state along with a percentage.  You can view and modify the options used for the display by selecting the paint brush icon at the bottom left of the display area (note this may be invisible until you hover near the area with your mouse).  If you change the value to city instead of state, you will see a busier graph.\n",
    "\n",
    "You can switch to a different dataframe at anytime by changing the value in the display parameter and rerunning the cell.  \n",
    "\n",
    "Change the display to use the california data display(df_cal).\n",
    "\n",
    "select the histogram chart from the drop down and then modify the values to contain city.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "dataframe",
      "keyFields": "STATE",
      "staticFigure": "false",
      "valueFields": "STATE"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### At this point we are ready to start our first pass at building a predictive model.  In this example we will use the statsmodel.formula.api package.  \n",
    "\n",
    "#### Use the following code to build a basic linear regression.\n",
    "\n",
    "    import statsmodels.formula.api as sm\n",
    "\n",
    "    result = sm.ols(formula=\"FATALS ~ VE_TOTAL + PERSONS + WEATHER + VE_FORMS\", data=pd_cal).fit()\n",
    "    print result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a summary of all the results, use\n",
    "\n",
    "    print result.summary()\n",
    "\n",
    "As you can see from the results, this is not a good model at all.  Select a different set of attributes to see if you can improve the R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}